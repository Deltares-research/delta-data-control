# Delta Data Control

A demonstration project showcasing version-controlled machine learning experiments using DVC (Data Version Control) with k-means clustering on temperature data.

## ğŸ“– Introduction

This is an example project demonstrating best practices for reproducible machine learning workflows. The focus is on **tooling and workflow reproducibility** rather than scientific complexity. The project uses real climate data from NOAA weather stations to perform k-means clustering on temperature patterns, illustrating how to version-control experiments, manage data artifacts, and track pipeline execution.

## ğŸ¯ Project Overview

The project implements a three-stage data pipeline:

1. **Data Collection** (`collect_data`) - Fetches historical temperature data from NOAA weather stations
2. **Data Processing** (`process_data`) - Performs k-means clustering on temperature data and computes quality metrics
3. **Visualization** (`visualize_results`) - Creates plots showing climate clusters and performance metrics

All stages are orchestrated through DVC with parameters managed in `params.toml`. The pipeline automatically tracks dependencies, ensuring reproducibility and efficient caching.

### Project Structure

```
delta-data-control/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ collection.py      # Data collection stage
â”‚   â”œâ”€â”€ process.py         # Clustering and metrics
â”‚   â””â”€â”€ visualize.py       # Visualization stage
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input.txt          # Generated temperature data
â”‚   â”œâ”€â”€ metrics.json       # Clustering quality metrics
â”‚   â””â”€â”€ image.png          # Cluster visualization
â”œâ”€â”€ params.toml            # Experiment parameters
â”œâ”€â”€ dvc.yaml               # Pipeline definition
â”œâ”€â”€ dvc.lock               # Pipeline state 
â”œâ”€â”€ pyproject.toml         # Project dependencies
â”œâ”€â”€ uv.lock                # uv lock file
â””â”€â”€ README.md              # Documentation
```

## ğŸ“‹ Requirements

### Package Manager
- **[uv](https://docs.astral.sh/uv/)** - Fast Python package and project manager

### Version Control
- **[Git](https://git-scm.com/)** - Distributed version control for code
- **[DVC](https://dvc.org/)** - Data Version Control for data and model artifacts

## âš™ï¸ DVC Setup

### Basic Setup

Initialize DVC in the project (already configured):

```cmd
dvc init
```

### Configure Remote Storage (MinIO)

This project uses a self-hosted MinIO bucket for remote storage. To configure it:

#### Step 1: Add DVC remote
```cmd
dvc remote add -d minio s3://field-data-flows/delta-data-control
```

#### Step 2: Configure MinIO endpoint

Replace `{YOUR_MINIO_URL}` with your MinIO server address:
```cmd
dvc remote modify minio endpointurl https://your-minio-server.com:9000
```

**Security Note:** If using an internal/private MinIO instance, be cautious about sharing this URL publicly.


#### Step 3: Add credentials (stored locally, not committed)
```cmd
dvc remote modify --local minio access_key_id {YOUR_ACCESS_KEY}
dvc remote modify --local minio secret_access_key {YOUR_SECRET_KEY}
```

#### Step 4: Configure S3 connection settings
```cmd
dvc remote modify minio ssl_verify false
dvc remote modify --local minio read_timeout 60
dvc remote modify --local minio connect_timeout 60
```

#### Step 5: Test the connection

To actually test the connection to MinIO:
```cmd
dvc push
```

If credentials or connection fails, you'll get an error. If successful (or says nothing to push), your connection is working.

**Credential Storage Options:**
- `.dvc/config.local` - Automatically gitignored (recommended for security)
- `.env` file - Use with `python-dotenv` in scripts (also add to `.gitignore`)
- Windows environment variables - System-wide, persistent

**Never commit credentials to Git or include them in `.dvc/config`.**

## ğŸš€ Quick Start

### Installation

```cmd
uv sync
```

### Run the Full Pipeline

Execute all stages in sequence:

```cmd
dvc repro
```

This will:
1. Collect synthetic temperature data
2. Run k-means clustering and compute metrics
3. Generate visualization plots

### Run Individual Stages

To run specific stages without executing the full pipeline:

```cmd
# Run only data collection
uv run src/collection.py

# Run only data processing
uv run src/process.py

# Run only visualization
uv run src/visualize.py
```

### View Pipeline Status

```cmd
dvc status      # Check which stages need re-running
dvc dag         # Visualize the pipeline DAG
```

## ğŸ“š More Information

### Configuration Parameters

Edit `params.toml` to modify experiment behavior:

```toml
[data]
# NOAA Climate Data API endpoint
url = "https://www.ncei.noaa.gov/access/services/data/v1"
dataset = "daily-summaries"
# Sample US weather stations
stations = ["USW00094728", "USW00023174", "USW00013874", "USW00012960", "USW00003017"]
start_date = "2023-01-01"
end_date = "2023-12-31"
# Temperature data fields
dataTypes = ["TMAX", "TMIN"]

[clustering]
n_clusters = 3
random_state = 42
max_iter = 300
n_init = 10

[visualization]
# Plot settings
figure_width = 12
figure_height = 8
dpi = 300
colormap = "viridis"

[output]
input_data = "data/input.txt"
metrics_file = "data/metrics.json"
visualization = "data/image.png"
```

After modifying parameters, run `dvc repro` to re-execute affected stages.

**Key Parameters:**
- `stations` - NOAA weather station IDs to fetch data from
- `start_date` / `end_date` - Date range for historical data
- `dataTypes` - Temperature metrics (TMAX, TMIN)
- `n_clusters` - Number of climate clusters for k-means
- `dpi` - Resolution of visualization output

### Pipeline Outputs

- **data/input.txt** - Historical temperature data from NOAA weather stations (TMAX and TMIN for selected date range)
- **data/metrics.json** - Clustering quality metrics (silhouette score, Davies-Bouldin index, etc.)
- **data/image.png** - Cluster visualization showing temperature patterns across regions

### Version Control Strategy

#### What Goes to Git
- âœ… Python scripts (`src/*.py`)
- âœ… Configuration (`params.toml`)
- âœ… Pipeline definition (`dvc.yaml`, `dvc.lock`)
- âœ… DVC metadata (`.dvc/config`, `.gitignore`)
- âœ… Project files (`pyproject.toml`, `README.md`)

#### What Goes to DVC
- âœ… Data files (`data/input.txt`)
- âœ… Metrics and outputs (`data/metrics.json`, `data/image.png`)
- âœ… Large datasets and model artifacts

#### What Gets Ignored
- âŒ Virtual environment
- âŒ Python cache (`__pycache__/`, `*.pyc`)
- âŒ Credentials and secrets (`.dvc/config.local`)

### Experiment Workflow

Follow this exact order to ensure reproducibility:

```cmd
# 1. Modify parameters or code
# Edit params.toml or src/*.py

# 2. Run the pipeline
dvc repro

# 3. Commit changes to Git
git add params.toml dvc.lock src/
git commit -m "Experiment: adjusted clustering parameters"

# 4. Push data to DVC remote (BEFORE git push)
dvc push

# 5. Push code to GitHub
git push
```

**Important:** Always run `dvc push` before `git push`. This ensures data is backed up to MinIO before Git metadata references it.
